{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation English to German Audio Experiment\n",
    "## Experiment 2\n",
    "### Text Translation 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Connor\\Tools\\envs\\hugFace\\lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Connor\\Tools\\envs\\hugFace\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Connor\\Tools\\envs\\hugFace\\lib\\site-packages\\transformers\\generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation_model_and_tokenizer(src_lang, dst_lang):\n",
    "  \"\"\"\n",
    "  Given the source and destination languages, returns the appropriate model\n",
    "  See the language codes here: https://developers.google.com/admin-sdk/directory/v1/languages\n",
    "  For the 3-character language codes, you can google for the code!\n",
    "  \"\"\"\n",
    "  # construct our model name\n",
    "  model_name = f\"Helsinki-NLP/opus-mt-{src}-{dst}\"\n",
    "  # initialize the tokenizer & model\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "  # return them for use\n",
    "  return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model Using Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Connor/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-de\\snapshots\\6183067f769a302e3861815543b9f312c71b0ca4\\config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"Helsinki-NLP/opus-mt-en-de\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      58100\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 58100,\n",
      "  \"decoder_vocab_size\": 58101,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 58100,\n",
      "  \"scale_embedding\": true,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 58101\n",
      "}\n",
      "\n",
      "loading file source.spm from cache at C:\\Users\\Connor/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-de\\snapshots\\6183067f769a302e3861815543b9f312c71b0ca4\\source.spm\n",
      "loading file target.spm from cache at C:\\Users\\Connor/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-de\\snapshots\\6183067f769a302e3861815543b9f312c71b0ca4\\target.spm\n",
      "loading file vocab.json from cache at C:\\Users\\Connor/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-de\\snapshots\\6183067f769a302e3861815543b9f312c71b0ca4\\vocab.json\n",
      "loading file target_vocab.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Connor/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-de\\snapshots\\6183067f769a302e3861815543b9f312c71b0ca4\\tokenizer_config.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\Connor/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-de\\snapshots\\6183067f769a302e3861815543b9f312c71b0ca4\\config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"Helsinki-NLP/opus-mt-en-de\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      58100\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 58100,\n",
      "  \"decoder_vocab_size\": 58101,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 58100,\n",
      "  \"scale_embedding\": true,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 58101\n",
      "}\n",
      "\n",
      "c:\\Users\\Connor\\Tools\\envs\\hugFace\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "loading configuration file config.json from cache at C:\\Users\\Connor/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-de\\snapshots\\6183067f769a302e3861815543b9f312c71b0ca4\\config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"Helsinki-NLP/opus-mt-en-de\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      58100\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 58100,\n",
      "  \"decoder_vocab_size\": 58101,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 58100,\n",
      "  \"scale_embedding\": true,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 58101\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Connor/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-de\\snapshots\\6183067f769a302e3861815543b9f312c71b0ca4\\pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      58100\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 58100,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"max_length\": 512,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 58100\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-de.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\Connor/.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-de\\snapshots\\6183067f769a302e3861815543b9f312c71b0ca4\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      58100\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 58100,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"max_length\": 512,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 58100,\n",
      "  \"renormalize_logits\": true\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# source & destination languages\n",
    "src = \"en\"\n",
    "dst = \"de\"\n",
    "\n",
    "model, tokenizer = get_translation_model_and_tokenizer(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7799, 39858,    20,   536,  1290,   268,  3977,   112,   268,   757,\n",
      "         18170,    27,    58,    14,   586,    13,  4904, 15823, 38818,     2,\n",
      "         10884, 20420,    12,    43,   128,     7,     4,  7833, 38818,     6,\n",
      "             7,    92,   160,     3, 39858,    19,   517,  1369,    23,  3121,\n",
      "             4,  8807,     7,  5049,   658,     2,   144,   137,   115,   319,\n",
      "           501,  6820,    12,     4,   478,     7,     4,  8807,     7, 35266,\n",
      "         35330,     3,   465,  1270, 24370,     8, 35266, 35330,    48,   848,\n",
      "             4,   254, 26364,     7,  1457, 19419,     3,  1704,  5131,  1211,\n",
      "         28077, 44216,  8418,   155,  2095,   285,   265,  2877,    86, 24430,\n",
      "            59,  5049,   658,  8807,     2,    99,   152,   143,  6165,   108,\n",
      "            47,   500,   360,    22,     6,   306,  3420, 28408,   400,  1704,\n",
      "           269,    19,   115,  1369,    23,   136,  4633,    32,     4, 12997,\n",
      "             7,  4802,     3,   231,  1795,     4, 27671, 29731, 12280,     5,\n",
      "         24468,    47,  1720,   154,   532,    12, 15823, 19419,     2,     8,\n",
      "          1452,    23,   154, 17853,     7,     4,   966,     7,     4,  3706,\n",
      "         20661,  1576,   327,    14, 47519,  2366,     5,     4,   478,     7,\n",
      "         35266,  8807,     3,  1704, 11281, 18520,     8,  1455,   658, 13480,\n",
      "             5,    47,   321,   118,  2329,    71,  6994, 46449,    33,    47,\n",
      "           749,  2365,    71,     0]])\n"
     ]
    }
   ],
   "source": [
    "article = \"\"\"\n",
    "Albert Einstein ( 14 March 1879 – 18 April 1955) was a German-born theoretical physicist, widely acknowledged to be one of the greatest physicists of all time. \n",
    "Einstein is best known for developing the theory of relativity, but he also made important contributions to the development of the theory of quantum mechanics. \n",
    "Relativity and quantum mechanics are together the two pillars of modern physics. \n",
    "His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been dubbed \"the world's most famous equation\". \n",
    "His work is also known for its influence on the philosophy of science.\n",
    "He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the development of quantum theory. \n",
    "His intellectual achievements and originality resulted in \"Einstein\" becoming synonymous with \"genius\"\n",
    "\"\"\"\n",
    "\n",
    "# encode the text into tensor of integers using the appropriate tokenizer\n",
    "inputs = tokenizer.encode(article, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Outputs vs Greedy Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam Outputs\n",
      "Albert Einstein (* 14. März 1879 – 18. April 1955) war ein deutscher theoretischer Physiker, der als einer der größten Physiker aller Zeiten anerkannt wurde. Einstein ist am besten für die Entwicklung der Relativitätstheorie bekannt, aber er leistete auch wichtige Beiträge zur Entwicklung der Quantenmechaniktheorie. Relativität und Quantenmechanik sind zusammen die beiden Säulen der modernen Physik. Seine Massenenergieäquivalenzformel E = mc2, die aus der Relativitätstheorie hervorgeht, wurde als „die berühmteste Gleichung der Welt\" bezeichnet. Seine Arbeit ist auch für ihren Einfluss auf die Philosophie der Wissenschaft bekannt. Er erhielt 1921 den Nobelpreis für Physik „für seine Verdienste um die theoretische Physik und vor allem für seine Entdeckung des Gesetzes über den photoelektrischen Effekt\", einen entscheidenden Schritt in der Entwicklung der Quantentheorie. Seine intellektuellen Leistungen und Originalität führten dazu, dass „Einstein\" zum Synonym für „Genius\" wurde.\n",
      "Greedy Outputs\n",
      "Albert Einstein (* 14. März 1879 – 18. April 1955) war ein deutscher theoretischer Physiker, der allgemein als einer der größten Physiker aller Zeiten anerkannt wurde. Einstein ist am besten für die Entwicklung der Relativitätstheorie bekannt, aber er leistete auch wichtige Beiträge zur Entwicklung der Quantenmechaniktheorie. Relativität und Quantenmechanik sind zusammen die beiden Säulen der modernen Physik. Seine Massenenergieäquivalenzformel E = mc2, die aus der Relativitätstheorie hervorgeht, wurde als „die berühmteste Gleichung der Welt\" bezeichnet. Seine Arbeit ist auch für ihren Einfluss auf die Philosophie der Wissenschaft bekannt. Er erhielt 1921 den Nobelpreis für Physik „für seine Verdienste um die theoretische Physik und vor allem für seine Entdeckung des Gesetzes über den photoelektrischen Effekt\", einen entscheidenden Schritt in der Entwicklung der Quantentheorie. Seine intellektuellen Leistungen und Originalität führten dazu, dass „Einstein\" zum Synonym für „Genius\" wurde.\n"
     ]
    }
   ],
   "source": [
    "# generate the translation output using beam search\n",
    "beam_outputs = model.generate(inputs, num_beams=3)\n",
    "# decode the output and ignore special tokens\n",
    "print(\"Beam Outputs\")\n",
    "print(tokenizer.decode(beam_outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# generate the translation output using greedy search\n",
    "greedy_outputs = model.generate(inputs)\n",
    "# decode the output and ignore special tokens\n",
    "print(\"Greedy Outputs\")\n",
    "print(tokenizer.decode(greedy_outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugFace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
